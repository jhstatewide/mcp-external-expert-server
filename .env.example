# MCP Delegate Server Configuration
# Copy this file to .env and fill in your values
# The .env file is gitignored and will not be committed

# Provider selection (ollama or openai_compat)
DELEGATE_PROVIDER=ollama
DELEGATE_BASE_URL=http://localhost:11434
DELEGATE_MODEL=qwen2.5:14b-instruct

# OpenAI-compatible only (leave empty for Ollama)
DELEGATE_API_KEY=
DELEGATE_OPENAI_PATH=/v1/chat/completions

# Behavior settings
# Timeout for API calls in milliseconds (default: 60000 = 60 seconds)
# Increase this if your Ollama server is slow (e.g., 300000 for 5 minutes)
DELEGATE_TIMEOUT_MS=60000
DELEGATE_MAX_TOKENS=800
DELEGATE_TEMPERATURE=0.2
# Extract <think> tags from model responses (default: true)
# Set to false to return raw responses without extracting thinking
DELEGATE_EXTRACT_THINKING=true

# Optional per-mode system prompts (leave empty to use defaults)
# DELEGATE_SYSTEM_PLAN="..."
# DELEGATE_SYSTEM_CRITIC="..."
# DELEGATE_SYSTEM_CRITIQUE="..."  # Devil's advocate mode - find flaws and weaknesses
# DELEGATE_SYSTEM_TESTS="..."
# DELEGATE_SYSTEM_EXPLAIN="..."

# MCP transport toggles
# MCP_HTTP enables HTTP/SSE transport (supports MCP Inspector, Goose Desktop, Cursor)
MCP_HTTP=false
MCP_HTTP_PORT=3333
MCP_STDIO=true
